---
title: "README"
output: html_document
---

# COMMIT SUMMARY
We have updated the code, thank you for your help Nico. We have tarted with the stratification. We are struggling to understand it, but the code works. Please see questions in the relevant section.

We have compiled the code for running multiple regressions. We have some questions (which follow the relevant section below)


# Aim

The aim of this paper is to find the best hedge for a Rand depreciation

# Techniques used

We will use a parametric (DCC) and a non-parametric measure of a hedge (stratifying the data set)

# NICO: Remember what I said in class - have a parametric (DCC) and also a non-parametric measure of a hedge (stratifying your data set)

# I'm happy for you to use the Dollar and e.g. Euro and Pound exchange rates - not always so sure about the currency's relative strength indices. Practitioners are mostly interested in ZARUSD.


# Data

## Data Prep

First we loaded the data ReturnsData and ETFs, respectively.

```{r Code for loading data}
library(tidyverse)
# NicoCOmment: add all the needed libraries!
library(rmsfuns)
library(lubridate)

etfs <-
  readRDS("data/AllFunds.rds") %>% tbl_df()

data_original <-
  readRDS("data/SA_Rand_Returns.rds")


spots <-
  readRDS("data/Spots.rds")

# NicoCOmment: You've not picked up that there remains weekends in this data file. Try and be sharp in your analysis - this study has potential but I need to be able to trust your work.
# I've done this in a tut before:
spots <- 
  spots %>% 
  mutate(Days = format(date, "%A")) %>% filter(!Days %in% c("Saturday", "Sunday") ) %>% select(-Days)
  
```


After this, we calculated the returns and merged data
QUESTION: NICO: Please can you check how I calculated returns for usdzar.
NicoCOmment: Calculated Correctly.

```{r Calculating returns and merging datasets}

N_Capping <- 80 # Parameter that trims the universe set. Focus, e.g., on the top 80 stocks by Market Cap.

ETFReturns <-
etfs %>% group_by(Ticker) %>% 
  rename("TRI" = TOT_RETURN_INDEX_NET_DVDS) %>% 
  mutate(Return = TRI / lag(TRI)-1) %>% ungroup()

SAData_Returns <-   
data_original %>% 
  filter(Universe == "JALSHAll") %>% 
  mutate(Return = coalesce(Return, 0) ) %>%   # To make NA's zero - check whether this fits in to your study / makes sense --> motivate.
  ungroup() %>% select(date, Ticker, BICS_LEVEL_1_SECTOR_NAME, Market.Cap, Return) %>% 
  group_by(date) %>% 
  arrange(date, Market.Cap) %>% 
  top_n(N_Capping, "Market.Cap") %>% ungroup()

# Caluclating returns for USDZAR:

usdzar <- 
  spots %>% group_by(Spot) %>% 
  mutate(Return = Value/lag(Value)-1) %>%  
  filter(Spot == "ZAR_Spot") %>% 
  ungroup()

# And here's how to look at cumulative returns for e.g. Naspers:

SAData_Returns %>% filter(Ticker == "NPN SJ Equity") %>% mutate(cumret = cumprod(1+Return)*100) %>% ggplot() + geom_line(aes(date, cumret))


# Merging datasets:
mergeddataset <- 
  bind_rows(
    ETFReturns %>% select(date, Ticker, Return),
    SAData_Returns %>% select(date, Ticker, Return),
    usdzar %>% rename("Ticker" = Spot) %>% select(date, Ticker, Return)
  )

```

# Stratification

## Stratifying our data

QUESTION: The values created (under Df) - what are they based on/how are they genereated? We are just not sure where they come from, seeing as there is only dates in the function. 

NicoCOmment: Guys, seriously - I created Df below as an illustration. You are supposed to replace Df below with ZAR. You need to take the project and make it your own - I cannot do everything. See below.


```{r Stratification}
Df <-
  data.frame(
    date = dateconverter(ymd(20100101), ymd(20170901), Transform = "weekdayEOW"),
    Value = runif(length(dateconverter(ymd(20100101), ymd(20170901), Transform = "weekdayEOW")))
  )

StratValue1 <- 0.2 # Change threshold
StratValue2 <- 0.8 # Change threshold

df_Strat <- 
  Df %>% 
  mutate(Q1 = quantile(Value, StratValue1, na.rm = TRUE), Q2 = quantile(Value, StratValue2, na.rm = TRUE)) %>% 
  mutate(ID = ifelse(Value <= Q1, "Low", 
                     ifelse(Value > Q1 & Value <= Q2 , "Medium",
                            ifelse(Value > Q2 , "High", "NA")) )) %>% ungroup() 



HighDates <- df_Strat
LowDates <- df_Strat


# Now use these dates to truncate your sample in order to reflect your strata of choice.

# E.g., in your regression function:

# df %>% filter(date %in% HighDates) %>% ...  






# NicoCOmment: This is how to use the above.

Df <- 
  usdzar %>% select(date, Return) %>% filter(date > first(date))

StratValue1 <- 0.2 # Change threshold
StratValue2 <- 0.8 # Change threshold

df_Strat <- 
  Df %>% 
  mutate(Q1 = quantile(Return, StratValue1, na.rm = TRUE), Q2 = quantile(Return, StratValue2, na.rm = TRUE)) %>% 
  mutate(ID = ifelse(Return <= Q1, "Low", 
                     ifelse(Return > Q1 & Return <= Q2 , "Medium",
                            ifelse(Return > Q2 , "High", "NA")) )) %>% ungroup() 



HighDates <- df_Strat %>% filter(ID == "High") %>% pull(date)
LowDates <- df_Strat %>% filter(ID == "Low") %>% pull(date)


# And now you can do the following e.g.:
Regression_data <-   mergeddataset

Regression_data %>% filter(date %in% HighDates)


# AGAIN: THIS IS ONLY an illustration. Use and tailor this to fit your own methodology - be creative in thinking how to use this. E.g., you might consider working with weekly returns after a high ZAR movement / weekly movement, or the day after a high return movement (the latter would require lagging your stock / ETF returns).

# I don't want to prescribe (this is ultimately your project) but think along the lines of:

# Returns day after high ZAR move - measures most immediately sensitive to ZAR movement.

# You could also use the return a week's return after a high currency movement (week movement).

# E.g. you could use the fornmat function above (used to trim out weekends) to calculate returns on a week to week basis (filter Days == "Wednesday" e.g.) and then look at ZAR movement's impact on same week stock movement. Here you won't lag.

# Please guys, let your mind go on this.


```

# Regression Time

Code for this section follows in 'code/regressions.R'. Need to run 'code/data_prep.R' to get mergeddataset. Could we make this more efficient?

```{r Regressions}
# First off... -------------------------------------------------------------
#   Need to run 'code/data_prep.R' to obtain the mergeddataset

Regression_data <-   mergeddataset

# Packages ----------------------------------------------------------------
library(rmsfuns)
packages_reg <- c("broom")
load_pkg(packages_reg)


# Data prep ---------------------------------------------------------------

zar <- usdzar %>% select("date" , "Return") %>% rename("usdzar_spot" = Return) 

Regression_data <- 
  right_join(Regression_data, zar, by = "date") %>% 
  
  filter(Ticker != "ZAR_Spot") %>% 
  filter(!is.na(Return))



# REGRESSION ANALYSIS
# Running multiple regressions --------------------------------------------

head(Regression_data)


Regressions <- 
  Regression_data %>%
  group_by(Ticker) %>% 
  do(reg = lm(usdzar_spot ~ (Return), data = .)) ##*THIS IS WHAT MY QUESTION REFERS TO*## 

RegressionCoeffs <- 
  Regressions %>% tidy(reg)

head(RegressionCoeffs)


# Tidy output for the paper -----------------------------------------------

load_pkg("huxtable")

variable.names <- unique(Regression_data$Ticker, incomparables = FALSE) #**** WHAT SHOULD WE INCLUDE HERE? LEEU?  ****

Title <- "Regression Table"


ht <- 
  huxreg(Regressions %>% filter(Ticker %in% variable.names ) %>% 
           select(reg) %>% .[[1]], 
         statistics = c(N = "nobs", R2 = "r.squared"), 
         note = "%stars%." )

#*** This takes a while to run (1-2 mins)
for(i in 1:ncol(ht)) {
  ht[1,][[1+i]] <- variable.names[i]  
}

ht %>% 
  set_caption(Title)




```
QUESTIONS:

\item What exactly should be regressed? (see comment in code above). We are struggling to get the usdzar as our dependent variable. We are not sure if the zar spot should form part of the returns column, or have its own (as we have done). _It is now correct._

# DCC
Here, we are struglling to get going. 
QUESTIONS:
\item Can't calculate the dlog returns. Tried various methods, but got strange answers because returns are zero/negative and give NAs or infinity when logged.
\item This has tripped us up and can't continue until we fix this

```{r DCC}

# Data --------------------------------------------------------------------

dailydata <- mergeddataset


# MV Conditional Heteroskedasticity tests ---------------------------------
# *** Cannot get out code to work here
# dlog returns: 

# NicoComment: SERIOUSLY guys, you are better than this. Below you are taking the dlog return of a return series...?! Just keep it in simple returns. 
rtn <- (
  diff( log(dailydata %>% arrange(date) %>% tbl_xts()), lag=1)
)*100


#drop the first observation and corresponding date:
rtn <- rtn[-1,]
# Center the data:
rtn <- scale(rtn,center=T,scale=F) 

colnames(rtn) <- 
  colnames(rtn) %>% gsub("JSE.","",.) %>% gsub(".Close","",.)

# And clean it using Boudt's technique:
rtn <- PerformanceAnalytics::Return.clean(rtn, method = c("none", "boudt", "geltner")[2], alpha = 0.01)

## Running the MV Conditional Heteroskedasticity test
MarchTest(rtn)

# *************************************************************************************************
# *************************************************************************************************
#******************* WE GOT THIS FAR AND GOT STUCK. REST IS COPY & PASTE FROM TUT 7 ***************
# *************************************************************************************************
# *************************************************************************************************
# *************************************************************************************************



# Fitting DCC -------------------------------------------------------------

DCCPre <- dccPre(rtn/100, include.mean = T, p = 0)

# If you want to fit other univariate garch models for each series, use the fGarch package to do so.

names(DCCPre)

# We now have the estimates of volatility for each series. 
# Follow my lead below in changing the output to a usable Xts series for each column in rtn:
Vol <- DCCPre$marVol
colnames(Vol) <- colnames(rtn)
Vol <- 
  data.frame( cbind( date = index(rtn), Vol)) %>% # Add date column which dropped away...
  mutate(date = as.Date(date)) %>%  tbl_df()  # make date column a date column...
TidyVol <- Vol %>% gather(Stocks, Sigma, -date)
ggplot(TidyVol) + geom_line(aes(x = date, y = Sigma, colour = Stocks))


# ------- Back to DCC:

# After saving now the standardized residuals:
StdRes <- DCCPre$sresi
# We can now use these sresids to calculate the DCC model.

# BUT FIRST NOTE THIS:
# Now, here follows a CLASSIC example of bad names for package commands. 
# If you run the dccFit command, notice that it gives you an error for filter_ 

# Upon investigation (Cntrl + click on the word dccFit in Rstudio) - you will notice
# the dccFit function uses the command filter. The problem is, dplyr also uses filter.
# The MTS authors should have wrapped the command as stats::filter and not filter, as it 
# produces ambiguity between stats::filter and dplyr::filter...
# Try it yourself - Run the following code:
# DCC <- dccFit(StdRes, type="Engle")

# SO... to solve this petty issue, let's detach the tidyr and dplyr packages, 
# then run dccFit and then reload tidyr and dplyr...  
# (Note this takes a few minutes - go get coffee in the meantime):

detach("package:tidyr", unload=TRUE)
detach("package:tbl2xts", unload=TRUE)
detach("package:dplyr", unload=TRUE)
DCC <- dccFit(StdRes, type="Engle")

# Letâ€™s plot all the bivariate time-varying correlations with RMH from our DCC model:
Rhot <- DCC$rho.t
# Right, so it gives us all the columns together in the form:
# X1,X1 ; X1,X2 ; X1,X3 ; ....

# So, let's be clever about defining more informative col names. 
# I will create a renaming function below:

renamingdcc <- function(ReturnSeries, DCC.TV.Cor) {
  
  ncolrtn <- ncol(ReturnSeries)
  namesrtn <- colnames(ReturnSeries)
  paste(namesrtn, collapse = "_")
  
  nam <- c()
  xx <- mapply(rep, times = ncolrtn:1, x = namesrtn)
  # Now let's be creative in designing a nested for loop to save the names corresponding to the columns of interest.. 
  
  # TIP: draw what you want to achieve on a paper first. Then apply code.
  
  # See if you can do this on your own first.. Then check vs my solution:
  
  nam <- c()
  for (j in 1:(ncolrtn)) {
    for (i in 1:(ncolrtn)) {
      nam[(i + (j-1)*(ncolrtn))] <- paste(xx[[j]][1], xx[[i]][1], sep="_")
    }
  }
  
  colnames(DCC.TV.Cor) <- nam
  
  # So to plot all the time-varying correlations wrt SBK:
  # First append the date column that has (again) been removed...
  DCC.TV.Cor <- 
    data.frame( cbind( date = index(ReturnSeries), DCC.TV.Cor)) %>% # Add date column which dropped away...
    mutate(date = as.Date(date)) %>%  tbl_df() 
  
  DCC.TV.Cor <- DCC.TV.Cor %>% gather(Pairs, Rho, -date)
  
  DCC.TV.Cor
  
}

# Let's see if our function works! Excitement!
Rhot <- 
  renamingdcc(ReturnSeries = rtn, DCC.TV.Cor = Rhot)

head(Rhot)


# DCC: flexible Univariate specs ------------------------------------------

# Using the rugarch package, let's specify our own univariate functions to be used in the dcc process:

# Step 1: Give the specficiations to be used first:

# A) Univariate GARCH specifications:
uspec <- ugarchspec(variance.model = list(model = "gjrGARCH", garchOrder = c(1, 1)), 
                    mean.model = list(armaOrder = c(1, 0), include.mean = TRUE), 
                    distribution.model = "sstd")
# B) Repeat uspec n times. This specification should be self-explanatory...
multi_univ_garch_spec <- multispec(replicate(ncol(rtn), uspec))

# Right, so now every series will have a GJR Garch univariate specification. (see ?ugarchspec for other options...)

# C) DCC Specs
spec.dcc = dccspec(multi_univ_garch_spec, 
                   dccOrder = c(1, 1), 
                   distribution = 'mvnorm',
                   lag.criterion = c("AIC", "HQ", "SC", "FPE")[1],
                   model = c("DCC", "aDCC")[1]) # Change to aDCC e.g.

# D) Enable clustering for speed:
cl = makePSOCKcluster(10)

# ------------------------
# Step 2:
# The specs are now saved. Let's now build our DCC models...
# ------------------------

# First, fit the univariate series for each column: 
multf = multifit(multi_univ_garch_spec, rtn, cluster = cl)

# Now we can use multf to estimate the dcc model using our dcc.spec:
fit.dcc = dccfit(spec.dcc, 
                 data = rtn, 
                 solver = 'solnp', 
                 cluster = cl, 
                 fit.control = list(eval.se = FALSE), 
                 fit = multf)

# And that is our DCC fitted model!

# We can now test the model's fit as follows:
#   Let's use the covariance matrices to test the adequacy of MV model in fitting mean residual processes:
RcovList <- rcov(fit.dcc) # This is now a list of the monthly covariances of our DCC model series.
covmat = matrix(RcovList,nrow(rtn),ncol(rtn)*ncol(rtn),byrow=TRUE)
mc1 = MCHdiag(rtn,covmat)

# ....Check Tsay (2014 on the interpretation of these Portmanteau tests)....

# Now to save the time-varying correlations as specified by the DCC model, 
# it again requires some gymnastics from our side.
# First consider what the list looks like:
dcc.time.var.cor <- rcor(fit.dcc)
print(dcc.time.var.cor[,,1:3])


# Okay, so this format presents a particular challenge in 
# getting our time-varying series into a plottable format...
# Every date is a list, and the list a matrix... How to get it in
# a tidy and plottable format?!!

# If you're up for the challenge - see if you can solve this 
# little conundrum of getting the lists as bivariate pair columns before using my solution.

# -------------SOLUTION:--------------------

# We will first use the base R function aperm - which transposes any array 
# into a list by changing the dimensions
dcc.time.var.cor <- aperm(dcc.time.var.cor,c(3,2,1))
dim(dcc.time.var.cor) <- c(nrow(dcc.time.var.cor), ncol(dcc.time.var.cor)^2)

# And now we can rename our columns the same way as before. 
# Luckily we wrote a function so we can use it again...

dcc.time.var.cor <-
  renamingdcc(ReturnSeries = rtn, DCC.TV.Cor = dcc.time.var.cor)

# Note that the figure is very similar to our earlier DCC TV correlations.
# So having a GJR univariate model didn't exactly change much...

g1 <- 
  ggplot(dcc.time.var.cor %>% filter(grepl("SBK_", Pairs ), !grepl("_SBK", Pairs)) ) + 
  geom_line(aes(x = date, y = Rho, colour = Pairs)) + 
  theme_hc() +
  ggtitle("Dynamic Conditional Correlations: SBK")

print(g1)



```
 


# Miscellaneous Notes


***

# Paper

## Introduction

## Methodology

## Literature Review

## Results

## Conclusion


