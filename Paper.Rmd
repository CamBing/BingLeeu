---
# IMPORTANT: Change settings here, but DO NOT change the spacing. 
# Remove comments and add values where applicable. 
# The descriptions below should be self-explanatory

title: "Finding the Best Rand Hedge"
#subtitle: "This will appear as Right Header"

documentclass: "elsarticle"

# Comment: ----- Follow this pattern for up to 5 authors
Author1: "Cameron Bing"  # First Author
Ref1: "Stellenbosch University, Cape Town, South Africa" # First Author's Affiliation
Email1: "17140552\\@sun.ac.za" # First Author's Email address

Author2: "Leeuwner Esterhuysen"
Ref2: "Stellenbosch University, Cape Town, South Africa"
Email2: "leeuwner.93\\@gmail.com"
CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

#Author3: "John Doe"
#Email3: "JohnSmith\\@gmail.com"

CorrespAuthor_1: TRUE  # If corresponding author is author 3, e.g., use CorrespAuthor_3: TRUE

keywords: "DCC \\sep Multiple Regression Analysis \\sep Rand Hedge" # Use \\sep to separate
JELCodes: "L250 \\sep L100"

# Comment: ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage\\" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# Setting page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top

HardSet: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashong text to fit on pages, e.g. This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper. 
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
RemovePreprintSubmittedTo: TRUE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: no                         # Add a table of contents
numbersections: yes             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.
output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
    include:
      in_header: Tex/packages.txt # Reference file with extra packages
abstract: |
  This paper employs a multiple regression and DCC approach to identify optimal hedges against volatility in the Rand exchange rate. The approach allows for both fixed and time-varying methods, and identifies the top 10 stocks and ETFs readily available to investors on the Johannesburg Stock Exchange.
---

<!-- First: Set your default preferences for chunk options: -->

<!-- If you want a chunk's code to be printed, set echo = TRUE. message = FALSE stops R printing ugly package loading details in your final paper too. I also suggest setting warning = FALSE and checking for warnings in R, else you might find ugly warnings in your paper. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
# Note: Include = FALSE implies the code is executed, but not printed in your pdf.
# warning and message = FALSE implies ugly messages and warnings are removed from your pdf. These should be picked up when you execute the command chunks (code sections below) in your rmd, not printed in your paper!

# Libraries ---------------------------------------------------------------
library(tidyverse)
library(rmsfuns)
library(lubridate)
library(broom)
library(rugarch)
library(rmgarch)
library(tbl2xts)
library(MTS)
library(PerformanceAnalytics)
library(ggplot2)
library(ggthemes)
# Loading data ------------------------------------------------------------
etfs <-
  readRDS("data/AllFunds.rds") %>% tbl_df()

data_original <-
  readRDS("data/SA_Rand_Returns.rds")

spots <-
  readRDS("data/Spots.rds") %>% 
  mutate(Days = format(date, "%A")) %>% filter(!Days %in% c("Saturday", "Sunday") ) %>% select(-Days)

# Merging and Calculating returns -----------------------------------------------------


N_Capping <- 80 # Parameter that trims the universe set. Focus, e.g., on the top 80 stocks by Market Cap.

ETFReturns <-
  etfs %>% group_by(Ticker) %>% 
  rename("TRI" = TOT_RETURN_INDEX_NET_DVDS) %>% 
  mutate(Return = TRI / lag(TRI)-1) %>% ungroup()

SAData_Returns <-   
  data_original %>% 
  filter(Universe == "JALSHAll") %>% 
  mutate(Return = coalesce(Return, 0) ) %>%   # To make NA's zero - check whether this fits in to your study / makes sense --> motivate.
  ungroup() %>% select(date, Ticker, BICS_LEVEL_1_SECTOR_NAME, Market.Cap, Return) %>% 
  group_by(date) %>% 
  arrange(date, Market.Cap) %>% 
  top_n(N_Capping, "Market.Cap") %>% ungroup()

# Caluclating returns for USDZAR:

usdzar <- 
  spots %>% group_by(Spot) %>% 
  mutate(Return = Value/lag(Value)-1) %>%  
  filter(Spot == "ZAR_Spot") %>% 
  ungroup()


# Merging datasets:
mergeddataset <- 
  bind_rows(
    ETFReturns %>% select(date, Ticker, Return),
    SAData_Returns %>% select(date, Ticker, Return),
    usdzar %>% rename("Ticker" = Spot) %>% select(date, Ticker, Return)
  )


# Dlog Returns:
mergeddataset <- 
  mergeddataset %>% arrange(date) %>% group_by(Ticker) %>% mutate(Return = coalesce(Return, 0)) %>% 
  mutate(Index = cumprod(1+Return) ) %>% 
  mutate(DlogReturn =  log(Index) - log( lag(Index))) %>% ungroup() %>% 
  mutate(DlogReturn = coalesce(DlogReturn, 0)) %>% select(-Index)

# Now choose which you want to use.

# Regression approach --------------------------------------------------------------
Regression_data <-   mergeddataset %>% select("date", "Ticker", "DlogReturn")


#zar <- usdzar %>% select("date" , "Return") %>% rename("usdzar_spot" = Return) 

zar <- mergeddataset %>% 
  filter(Ticker == "ZAR_Spot")%>%
  select("date", "DlogReturn")  %>% 
  rename("usdzar_spot" = DlogReturn)

Regression_data <- 
  right_join(Regression_data, zar, by = "date") %>% 
  filter(Ticker != "ZAR_Spot") %>% 
  filter(!is.na(DlogReturn))

#== === === === === === === === === === === === === === ===
# TRIM THE REGRESSION DATA!!
# Do this now or after it has been run for the DCC
#== === === === === === === === === === === === === === ===
Pct_Valid <- 0.9  # This can change of course. 70% valid data over period at least
StartDate <- ymd(20050101)
EndDate <- ymd(20171031)

mergeddataset <- 
  mergeddataset %>% filter(date >= StartDate & date <= EndDate) #trimming dates covered

mergeddataset[is.na(mergeddataset)] <- 0 # (Added) replace NAs in mergeddata

NDates <- length(unique(mergeddataset %>% pull(date)) ) 

Tickers_Active_At_Last <- 
  mergeddataset %>% 
  select(date, Ticker, DlogReturn) %>% 
  filter(date >= ymd(20171001)) %>% # checking if valid at most recent date 
  group_by(Ticker) %>% 
  mutate(N_Valid = ifelse(DlogReturn == 0, 0, 1) ) %>% 
  summarise(S = sum(N_Valid)) %>% 
  filter(S >0) %>% pull(Ticker)

Tickers_To_Hold <- 
  mergeddataset %>% 
  select(date, Ticker, DlogReturn) %>% 
  filter(Ticker %in% Tickers_Active_At_Last) %>% 
  group_by(Ticker) %>% 
  mutate(N_Valid = ifelse(DlogReturn == 0, 0, 1) ) %>% summarise(N_Valid_Pct = sum(N_Valid)/NDates) %>% 
  filter(N_Valid_Pct >= Pct_Valid) %>% pull(Ticker) %>% unique()


regression.data.final <-  mergeddataset %>% 
  select(date, Ticker, DlogReturn) %>% 
  filter(date >= StartDate) %>% 
  filter(Ticker %in% Tickers_To_Hold) 

regression.data.final[is.na(regression.data.final)] <- 0 

regression.data.final <- 
  right_join(regression.data.final, zar, by = "date") %>% 
  filter(Ticker != "ZAR_Spot") %>% 
  filter(!is.na(DlogReturn))

#== === === === === === === === === ===
# REGRESSION ANALYSIS (NEW)
#== === === === === === === === === ===

Regressions <- 
  regression.data.final %>%
  group_by(Ticker) %>% 
  do(reg = lm(DlogReturn ~ (usdzar_spot), data = .)) 

RegressionCoeffs <- 
  Regressions %>% tidy(reg)

head(RegressionCoeffs)

hedges <- RegressionCoeffs %>%  
  filter(., term == "usdzar_spot") %>% 
  select(., Ticker, estimate) %>% 
  arrange(., desc(estimate))
#***** ^^ Try put into table ^^ ********

top.10.reg <-
  (RegressionCoeffs %>%  
     filter(., term == "usdzar_spot") %>% 
     select(., Ticker, estimate) %>% 
     arrange(., desc(estimate)))[1:10,]

# Tidy output for the paper 

load_pkg("huxtable")

variable.names <- unique(Regression_data$Ticker, incomparables = FALSE) #**** WHAT SHOULD WE INCLUDE HERE? LEEU?  ****

Title <- "Regression Table"


ht <- 
  huxreg(Regressions %>% filter(Ticker %in% variable.names ) %>% 
           select(reg) %>% .[[1]], 
         statistics = c(N = "nobs", R2 = "r.squared"), 
         note = "%stars%." )

#*** This takes a while to run (1-2 mins)
for(i in 1:ncol(ht)) {
  ht[1,][[1+i]] <- variable.names[i]  
}

ht %>% 
  set_caption(Title)



```


<!-- ############################## -->
<!-- # Start Writing here: -->
<!-- ############################## -->

# Introduction \label{Introduction}

\pagebreak

# Literature Review \label{Lit_Review}

The concurrent volatile nature of the South African Rand has brought about a widespread search for the best strategy aimed at protecting capital against exchange rate volatility. This paper makes use of @baur2010 's definitions of so-called ‘safe havens’ and ‘hedges’. They define a safe haven as an asset that is negatively related to another asset or groups of assets during periods of high market volatility. Furthermore, they define a hedge as an asset that that is negatively related to another asset or groups of assets, on average.
In terms of the South African equity market, there are various equities that may potentially provide protection against rand weakness, and hence act as a rand hedge. This is due to the fact that a significant share of companies listed on the Johannesburg Stock Exchange (JSE) has significant offshore exposure, either through selling products and services that are denominated in foreign currencies, or through significant offshore operations. As a result, such companies will experience an increase in rand-denominated revenue during periods where the rand depreciates. In theory, these increases in revenue should increase the value of these companies and consequently lead to higher share price valuations. This phenomenon subsequently results in a positive statistical relationship between the depreciation of the rand and the appreciation of the relevant share price, indicating the rand hedge potential of such a share. Another potential rand hedge strategy involves the purchasing of commodities. Since commodities are priced in dollars, their value increases as the rand weakens, hence serving as a hedge against the depreciation of the rand. 
Prior research on this topic in South Africa is relatively limited. @barr2007 made use of a regression model in order to investigate the relationships between the top 40 shares listed on the JSE and the rand-dollar exchange rate. The findings of their study imply that certain local equities can be compiled into a given domestic portfolio that could serve as an effective and consistent hedge against rand weakness. The same authors applied a GARCH regression approach in 2007 to study the relation between the same two variables: the top 40 shares listed on the JSE and the rand-dollar exchange rate. In this study, their findings indicate significant variations in the correlations in the correlations between the rand-dollar exchange rate and various shares. Some shares, however, are identified as effective hedges against rand depreciation (see @barr2007). 
There exists a vast international literature on the practical application of studying co-movements between various financial returns series in an attempt to hedge an investment portfolio against currency fluctuation. @fang2002 employed a bivariate GARCH-M model in order to study the co-movements between stock market returns and currency depreciation. Their findings suggest that some degree of temporal dependence between the conditional variance of currency depreciation and stock market returns. @mukherjee1995 and @kearney1998 find corroborating results, with their respective findings suggesting a cointegrating relationship between stock market returns and the exchange rate. 
The ability to understand and predict the temporal dependence in the second-order moments and to control for the second-order temporal persistence of asset returns, has various financial econometric applications [@bauwens2006]. @kennedy2016 state that increased exchange rate volatility leads to a statistically significant, positive impact on the volatility of stock market returns when the main sources of financial volatility are controlled for. The findings of @baur2010, who analysed the time-varying correlations between gold and a collection of other assets in Germany, the UK and US, suggest that gold serves as a safe haven for equities in all of these countries. @ciner2013 employed a DCC model with GARCH specification in order to determine the hedging ability of multiple assets against the British pound and US dollar. Their findings suggest that gold serves as a potential hedge against exchange rate volatility for both of these two currencies. 



\pagebreak

# Data \label{Data}


The data set used in this study includes the daily closing prices of an array of equties and ETFs which are traded and easily accessible to investors on the Johannesburg Stock Exchange. The top `r N_Capping` stocks, as measured by total market cap, which traded on at least 90\% of days between 01/01/2005 and 31/10/2017 were included in the analysis. Furthermore, an additional `r length(unique(etfs %>% pull(Ticker))) ` ETFs were included. This was then trimmed to include those ETFs which traded on at least 90\% of the aforementioned period. A full list of the assets covered in this analysis can be found in section \ref{appendix} in table \ref{cover}. In addition to these variables, the ZAR/USD exchange rate is included to be used as our variable of interest.

The continuously compounded daily sector returns are then calculated by taking the log difference of each index series, as:

\begin{align}
r _ { i ,t } = \operatorname{ln} ( \frac { p _ { i } ,t } { p _ { i ,t - 1} } ) * 100 
\end{align}

where $p_t$ represents the closing index price of asset $i$ at time $t$. Taking the first difference of these return series is then imposed to remove the unit root process evident in the data. This data is then used in the analysis which follows. 


```{r Figure1, warning =  FALSE, fig.align = 'center', fig.cap = "Caption Here \\label{Figure1}", fig.ext = 'png', fig.height = 3, fig.width = 6}


```



\pagebreak

#  Methodology
This study employs two methodologies to investigate which JSE-listed financial instruments provide the best hedge against volatility in the Rand exchange rate. The first method utilised is a regression model (\ref{regression}), following which a Dynamic Conditional Correlation (DCC) model \ref{dcc} is used to investigate time-varying corellations between various JSE-listed instruments and the Rand/US Dollar exchange rate. 

## Regression Model \label{regression}

A multiple regression approach was employed to investigate the static correlations between the Rand exchange rate and the various assets and financial instruments covered in our data set. The initial regression model, as specified as in equation \ref{eq1}, was run to investigate the relationship between the assets covered in the data set and the Rand/US Dollar exchange rate:

\begin{align} 
Return_t = \beta_0 + \beta_1 R_t + \epsilon_t \label{eq1}
\end{align}

where $Return_t$ refers to the first difference of the log returns of the assets and $R_t$ to the dlog Rand/US Dollar exchange rate returns at time $t$. This specification includes covers all dates within the data set.

Following these results, the data set was stratified in order to isolate the analysis to times of high volatility, both positive and negative, in the Rand exchange rate. This model was specified as follows:

\begin{align} 
Return_t = \beta_0 + \beta_1 R.pos.vol_t + \beta_2 R.neg.vol_t + \epsilon_t \label{eq2}
\end{align}

where $R.pos.vol_t$ and $R.neg.vol_t$ refer to dates where the Rand exchange rate experienced periods of high positive and negative volotility, repectively. The distinction between times of high and relatively low volatility is important as this study's findings will be most relevant to investors in times of high unstability in the Rand. Furthermore, it allows us to minimize noise in the study which may drive nonsensical results.

\pagebreak

## DCC Model

This study utilises a DCC Multivariate Generalized Autoregressive Conditional Heteroskedasticity (MV-GARCH) approach to isolate the time-varying conditional correlations between an array of JSE-listed stocks and ETFs. This technique offers a parsimonious approach to MV colatility modelling by relaxing the constraint of a fixed correlation structure which imposed in other modelling techniques. The results of which allow us to study whether fluctuations in the Rand exchange rate influence the aforementioned financial instruments. This information can then be reinterpreted as an indication of the best hedging options available to investors in the South African market. In contrast to the regression model, as described in section \ref{regression}, this method allows us to assess the dynamic hedging potential of the assets covered in our data set. 

The initial step in the DCC modelling process is to obtain univariate volatility for each series using a GARCH (1,1) process, specified as follows in \ref{eq3}:


\begin{align} 
  &r_t = \mu + \epsilon_t   \label{eq3}    \\ \notag 
  &varepsilon_t = \sigma_t . z_t \\ \notag
  &\sigma^2_t = \alpha + \beta_1 \epsilon^2_t-1 + \beta_1\sigma^2_t-1 \\ \notag
  &z_t \sim N(0,1) \notag
\end{align}

The dynamic conditional corelations are then estimated using a log-likelihood approach using the standardised residuals extracted from the GARCH (1,1) process. 

The DCC model can be defined as:

\begin{align}
H_t = D_t.R_t.D_t \label{eq4}
\end{align}

where $H_t$ is the conditional covariance matrix of the stochastic process, $D_t$ is a diagonal matrix and $R_t$ refers to  the time-varying correlations between assets. Equation \ref{eq4} seperates the variance covariance matrix into identical diagonal matrices and an estimate of the time-varying correlation between assets. 

Estimating $R_t$ requires it to be inverted in each time period, for which we use a proxy equation as in @engle2002:

\begin{align}
Q _ { i j ,t } &= \overline { Q } + a ( z _ { t - 1} z ^ { \prime } t _ { t - 1} - \overline { Q } ) + b ( Q _ { i j ,t - 1} - \overline { Q } )   \label{eq5} \\ \notag
 &= ( 1- a - b ) \hat { Q } + a z _ { t - 1} z _ { t - 1} ^ { \prime } + b .Q _ { i j ,t - 1} \notag
\end{align}

Equation \ref{eq5}'s form is similar to that of a GARCH(1,1) process, with non-negative scalars $a$ and $b$, $Q _ { i j ,t }$ the unconditional sample variance estimate between series $i$ and $j$, and the unconditional matrix of standardized residuals from each univariate pair estimate $\overline { Q }$.We then use equation \ref{eq5} to esimate $R_t$ as:

\begin{align}
R _ { t } = \operatorname{diag} ( Q _ { t } ) ^ { - 1/ 2} Q _ { t } \cdot \operatorname{diag} ( Q _ { t } ) ^ { - 1/ 2} \label{eq6}
\end{align}

with the following bivariate elements,

\begin{align}
R _ { t } = \rho _ { i j ,t } = \frac { q _ { i } ,j ,t } { \sqrt { q _ { i i } ,t \cdot q _ { j } j ,t } } 
\end{align}

The resulting DCC model is then formulated as:

\begin{align}
\epsilon _ { t } &\sim N ( 0,D _ { t } \cdot R _ { t } \cdot D _ { t } ) \label{eq7}  \\ \notag
D^2_t &\sim Univariate\quad GARCH(1,1) \quad process \quad \forall (i,j),\quad i \ne j \\ \notag
z _ { t } &= D _ { t } ^ { - 1} \cdot \epsilon _ { t } \\ \notag
Q _ { t } &= \overline { Q } ( 1- a - b ) + a ( z _ { t } ^ { \prime } z _ { t } ) + b ( Q _ { t - 1} ) \\ \notag
R _ { t } &= \operatorname{Diag} ( Q _ { t } ^ { - 1} ) \cdot Q _ { t } .\operatorname{Din} g ( Q _ { t } ^ { - 1} )
\end{align}

Fitting this technique to our data implies a two-step approach. First, a univariate GARCH model is applied to the residuals of each of our VAR series' residuals $\alpha_t = z_t - \mu_t$. The volatility series $h_t$ is then estimated in step 2. These volatility series are then standardized (see equation \ref{eq8} below) and used in fitting a DCC model for $\eta _ { t }$.

\begin{align}
\eta _ { i ,t } = \frac { \alpha _ { i ,t } } { \sigma _ { i ,t } } \label{eq8}
\end{align}



\pagebreak

# Results



# Conclusion

# Appendix \label{appendix}

```{r LongTable, results = 'asis'}
cover <- unique(regression.data.final %>% pull(Ticker)) 
# here, run xtable on col.names(rtn to get list of stocks and etfs covered).
library(knitr)
kable(cover, caption = "Stocks and ETFs Covered in Analysis", col.names = "\\label{cover}Stocks and ETFs Covered in Analysis", align = 'c') 
```



<!-- Make title of bibliography here: -->
<!-- \newpage -->
# References  
